# Contributing to DeepCausalMMM ðŸ¤

Thank you for your interest in contributing to DeepCausalMMM! This document provides guidelines for contributing to our advanced Media Mix Modeling package.

## ðŸ† Project Philosophy

### **Zero Hardcoding Principle**
- **All parameters must be configurable** via `config.py`
- **No magic numbers** in model code
- **Dataset agnostic** - works on any MMM dataset
- **Learnable parameters** preferred over fixed constants

### **Performance First**
- **Proven configurations** should not be changed without extensive testing
- **Benchmark against baseline** before and after changes
- **Maintain generalization** with proper train/holdout validation
- **Document performance impact** in pull requests

### **Code Quality Standards**
- **Type hints** for all function parameters and returns
- **Comprehensive docstrings** with examples
- **Error handling** with informative messages
- **Modular design** with clear separation of concerns

## ðŸš€ Getting Started

### Development Setup

1. **Clone the repository**
```bash
git clone https://github.com/adityapt/deepcausalmmm.git
cd deepcausalmmm
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install in development mode**
```bash
pip install -e .
# Or install with development dependencies
pip install -e .[dev]
```

4. **Verify installation**
```bash
python -c "from deepcausalmmm import DeepCausalMMM, get_device; print('âœ… Package installed successfully!')"
```

5. **Run tests to ensure setup**
```bash
python -m pytest tests/ -v
```

## ðŸ“ Project Structure

```
deepcausalmmm/                      # Project root
â”œâ”€â”€ pyproject.toml                  # Package configuration and dependencies
â”œâ”€â”€ README.md                       # Documentation
â”œâ”€â”€ LICENSE                         # MIT License
â”œâ”€â”€ CHANGELOG.md                    # Version history and changes
â”œâ”€â”€ CONTRIBUTING.md                 # Development guidelines (this file)
â”œâ”€â”€ CODE_OF_CONDUCT.md              # Code of conduct
â”œâ”€â”€ CITATION.cff                    # Citation metadata for Zenodo/GitHub
â”œâ”€â”€ Makefile                        # Build and development tasks
â”œâ”€â”€ MANIFEST.in                     # Package manifest for distribution
â”‚
â”œâ”€â”€ deepcausalmmm/                  # Main package directory
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization and exports
â”‚   â”œâ”€â”€ cli.py                      # Command-line interface
â”‚   â”œâ”€â”€ exceptions.py               # Custom exception classes
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                       # âš ï¸ CRITICAL: Core model components
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Core module initialization
â”‚   â”‚   â”œâ”€â”€ config.py              # âš ï¸ CRITICAL: All configurations
â”‚   â”‚   â”œâ”€â”€ unified_model.py       # âš ï¸ CRITICAL: Main model architecture
â”‚   â”‚   â”œâ”€â”€ trainer.py             # ModelTrainer class for training
â”‚   â”‚   â”œâ”€â”€ data.py                # UnifiedDataPipeline for data processing
â”‚   â”‚   â”œâ”€â”€ scaling.py             # SimpleGlobalScaler for data normalization
â”‚   â”‚   â”œâ”€â”€ seasonality.py         # Seasonal decomposition utilities
â”‚   â”‚   â”œâ”€â”€ dag_model.py           # DAG learning and causal inference
â”‚   â”‚   â”œâ”€â”€ inference.py           # Model inference and prediction
â”‚   â”‚   â”œâ”€â”€ train_model.py         # Training functions and utilities
â”‚   â”‚   â””â”€â”€ visualization.py       # Core visualization components
â”‚   â”‚
â”‚   â”œâ”€â”€ postprocess/                # Analysis and post-processing
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Postprocess module initialization
â”‚   â”‚   â”œâ”€â”€ analysis.py            # Statistical analysis utilities
â”‚   â”‚   â”œâ”€â”€ comprehensive_analysis.py  # Comprehensive analyzer
â”‚   â”‚   â”œâ”€â”€ response_curves.py     # Non-linear response curve fitting (Hill equations)
â”‚   â”‚   â””â”€â”€ dag_postprocess.py     # DAG post-processing and analysis
â”‚   â”‚
â”‚   â””â”€â”€ utils/                      # Utility functions
â”‚       â”œâ”€â”€ __init__.py            # Utils module initialization
â”‚       â”œâ”€â”€ device.py              # GPU/CPU device detection
â”‚       â””â”€â”€ data_generator.py      # Synthetic data generation (ConfigurableDataGenerator)
â”‚
â”œâ”€â”€ examples/                       # Example scripts and notebooks
â”‚   â”œâ”€â”€ quickstart.ipynb           # Interactive Jupyter notebook for Google Colab
â”‚   â”œâ”€â”€ dashboard_rmse_optimized.py # Comprehensive dashboard with 14+ visualizations
â”‚   â””â”€â”€ example_response_curves.py  # Response curve fitting examples
â”‚
â”œâ”€â”€ tests/                          # Test suite
â”‚   â”œâ”€â”€ __init__.py                # Test package initialization
â”‚   â”œâ”€â”€ unit/                      # Unit tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_config.py         # Configuration tests
â”‚   â”‚   â”œâ”€â”€ test_model.py          # Model architecture tests
â”‚   â”‚   â”œâ”€â”€ test_scaling.py        # Data scaling tests
â”‚   â”‚   â””â”€â”€ test_response_curves.py # Response curve fitting tests
â”‚   â””â”€â”€ integration/               # Integration tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_end_to_end.py     # End-to-end integration tests
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ Makefile                   # Documentation build tasks
â”‚   â”œâ”€â”€ make.bat                   # Windows documentation build
â”‚   â”œâ”€â”€ requirements.txt           # Documentation dependencies
â”‚   â””â”€â”€ source/                    # Sphinx documentation source
â”‚       â”œâ”€â”€ conf.py               # Sphinx configuration
â”‚       â”œâ”€â”€ index.rst             # Documentation index
â”‚       â”œâ”€â”€ installation.rst      # Installation guide
â”‚       â”œâ”€â”€ quickstart.rst        # Quick start guide
â”‚       â”œâ”€â”€ contributing.rst      # Contributing guide
â”‚       â””â”€â”€ api/                  # API documentation
â”‚           â”œâ”€â”€ index.rst
â”‚           â”œâ”€â”€ core.rst
â”‚           â”œâ”€â”€ data.rst
â”‚           â”œâ”€â”€ trainer.rst
â”‚           â”œâ”€â”€ inference.rst
â”‚           â”œâ”€â”€ analysis.rst
â”‚           â”œâ”€â”€ response_curves.rst # Response curves API
â”‚           â”œâ”€â”€ utils.rst
â”‚           â””â”€â”€ exceptions.rst
â”‚
â””â”€â”€ JOSS/                           # Journal of Open Source Software submission
    â”œâ”€â”€ paper.md                   # JOSS paper manuscript
    â”œâ”€â”€ paper.bib                  # Bibliography
    â”œâ”€â”€ figure_dag_professional.png # DAG visualization figure
    â””â”€â”€ figure_response_curve_simple.png # Response curve figure
```

## ðŸ”§ Development Guidelines

### **1. Configuration Management**

**âœ… DO:**
```python
# In unified_model.py
def __init__(self, config: Dict[str, Any]):
    self.hidden_dim = config['hidden_dim']
    self.dropout = config['dropout']
    self.gru_layers = config['gru_layers']
```

**âŒ DON'T:**
```python
# NEVER hardcode values
def __init__(self):
    self.hidden_dim = 320  # âŒ HARDCODED
    self.dropout = 0.08    # âŒ HARDCODED
```

### **2. Model Architecture Changes**

**Before making changes to core model:**
1. **Benchmark current performance** on test dataset
2. **Create feature branch** with descriptive name
3. **Implement with config parameters** - no hardcoding
4. **Test extensively** on multiple datasets
5. **Document performance impact**

**Critical Files (Require Extra Care):**
- `core/unified_model.py` - Main model architecture
- `core/config.py` - All configurations
- `core/trainer.py` - Training optimization

### **3. Adding New Features**

**Feature Development Process:**
1. **Create issue** describing the feature and motivation
2. **Design with configurability** in mind
3. **Add config parameters** with sensible defaults
4. **Implement with type hints** and docstrings
5. **Add comprehensive tests**
6. **Update documentation**
7. **Benchmark performance impact**

**Example: Adding New Loss Function**
```python
# 1. Add to config.py
'use_focal_loss': False,
'focal_alpha': 0.25,
'focal_gamma': 2.0,

# 2. Implement in trainer.py with config
def _calculate_loss(self, predictions, targets):
    if self.config.get('use_focal_loss', False):
        return self._focal_loss(predictions, targets)
    return self._huber_loss(predictions, targets)

# 3. Add tests
def test_focal_loss_calculation():
    # Test implementation
```

### **4. Data Processing**

**Scaling and Normalization Rules:**
- **Media variables**: SOV (Share-of-Voice) scaling
- **Control variables**: Z-score normalization  
- **Seasonality**: Min-Max scaling per region (0-1)
- **Target variable**: Log1p transformation

**âœ… DO:**
- Use `UnifiedDataPipeline` for all data processing
- Ensure consistent train/holdout transformations
- Document scaling choices with business justification

**âŒ DON'T:**
- Apply different scaling to train/holdout
- Use hardcoded scaling parameters
- Skip data validation steps

### **5. Testing Requirements**

**All contributions must include:**
1. **Unit tests** for new functions/classes
2. **Integration tests** for feature workflows
3. **Performance regression tests**
4. **Documentation tests** (docstring examples)

**Test Categories:**
```bash
# Unit tests
python -m pytest tests/unit/

# Integration tests  
python -m pytest tests/integration/

# Performance tests
python -m pytest tests/performance/

# All tests
python -m pytest tests/ -v --cov=deepcausalmmm
```

### **6. Documentation Standards**

**All functions must have comprehensive docstrings:**
```python
def calculate_contributions(
    self, 
    media_data: torch.Tensor, 
    coefficients: torch.Tensor,
    transform_back: bool = True
) -> Dict[str, torch.Tensor]:
    """
    Calculate economic contributions for media channels.
    
    Args:
        media_data: Media spend data [regions, time, channels]
        coefficients: Learned coefficients [regions, time, channels]
        transform_back: Whether to apply inverse log1p transform
        
    Returns:
        Dict containing:
            - 'total_contributions': Total economic impact per channel
            - 'regional_contributions': Contributions by region
            - 'temporal_contributions': Contributions over time
            
    Example:
        >>> contributions = model.calculate_contributions(
        ...     media_data=processed_media,
        ...     coefficients=learned_coeffs,
        ...     transform_back=True
        ... )
        >>> print(f"Total impact: {contributions['total_contributions'].sum()}")
    """
```

## ðŸ§ª Performance Standards

### **Benchmark Requirements**

**Performance Standards:**
- **Holdout RÂ²**: Should demonstrate strong generalization
- **Performance Gap**: Training vs holdout gap should be minimal
- **RMSE**: Should show consistent improvement over baseline
- **Training Stability**: No coefficient explosion or divergence
- **Business Logic**: Contributions should be realistic and interpretable

**Before Merging:**
1. **Run full benchmark** on standard dataset
2. **Compare against baseline** performance
3. **Document any performance changes**
4. **Get approval** for performance degradation >2%

### **Performance Testing**
```python
# Example performance test
def test_model_performance_regression():
    """Ensure new changes don't degrade performance."""
    config = get_config()
    
    # Load standard test dataset
    data = load_benchmark_data()
    
    # Train model
    results = train_and_evaluate(config, data)
    
    # Assert performance thresholds
    assert results['holdout_r2'] >= 0.85
    assert results['performance_gap'] <= 0.10
    assert results['holdout_rmse'] <= 400000
```

## ðŸ”„ Pull Request Process

### **1. Pre-submission Checklist**
- [ ] **Code follows zero-hardcoding principle**
- [ ] **All tests pass** (`python -m pytest tests/`)
- [ ] **Performance benchmarks meet thresholds**
- [ ] **Documentation updated** (README, docstrings)
- [ ] **Type hints added** to new functions
- [ ] **Config parameters added** for new features
- [ ] **No linting errors** (`flake8 deepcausalmmm/`)

### **2. PR Description Template**
```markdown
## ðŸŽ¯ Purpose
Brief description of what this PR accomplishes.

## ðŸ”§ Changes Made
- List of specific changes
- New features added
- Bug fixes included

## ðŸ“Š Performance Impact
- Benchmark results before/after
- Any performance changes documented
- Justification for performance changes

## ðŸ§ª Testing
- Tests added/modified
- Test coverage maintained
- Performance regression tests included

## ðŸ“š Documentation
- README updated if needed
- Docstrings added/updated
- Examples provided for new features

## âš ï¸ Breaking Changes
- Any breaking changes listed
- Migration guide provided if needed
```

### **3. Review Process**
1. **Automated checks** must pass (tests, linting, performance)
2. **Code review** by maintainers
3. **Performance review** if core components changed
4. **Documentation review** for clarity and completeness
5. **Final approval** and merge

## ðŸ› Bug Reports

### **Issue Template**
```markdown
## ðŸ› Bug Description
Clear description of the bug.

## ðŸ”„ Steps to Reproduce
1. Step 1
2. Step 2
3. Step 3

## ðŸ“Š Expected Behavior
What should happen.

## ðŸ“Š Actual Behavior  
What actually happens.

## ðŸ”§ Environment
- Python version:
- PyTorch version:
- Operating System:
- DeepCausalMMM version:

## ðŸ“‹ Additional Context
- Error messages
- Stack traces
- Sample data (if applicable)
```

## ðŸ’¡ Feature Requests

### **Enhancement Template**
```markdown
## ðŸŽ¯ Feature Description
Clear description of the proposed feature.

## ðŸ”§ Use Case
Why is this feature needed? What problem does it solve?

## ðŸ’­ Proposed Implementation
High-level approach to implementing the feature.

## ðŸ“Š Performance Considerations
Any potential impact on model performance.

## ðŸ”— Related Issues
Links to related issues or discussions.
```

## ðŸ·ï¸ Code Style

### **Python Style Guidelines**
- **PEP 8** compliance with 100-character line limit
- **Type hints** for all function signatures
- **Descriptive variable names** (no abbreviations)
- **Consistent formatting** using `black` formatter

### **Naming Conventions**
- **Functions/variables**: `snake_case`
- **Classes**: `PascalCase`  
- **Constants**: `UPPER_SNAKE_CASE`
- **Private methods**: `_leading_underscore`

### **Import Organization**
```python
# Standard library imports
import os
import sys
from typing import Dict, List, Optional, Tuple

# Third-party imports
import torch
import torch.nn as nn
import pandas as pd
import numpy as np

# Local imports
from deepcausalmmm.core.config import get_default_config
from deepcausalmmm.utils.device import get_device
```

## ðŸŽ‰ Recognition

Contributors will be recognized in:
- **CHANGELOG.md** for each release
- **README.md** contributors section
- **GitHub releases** with contributor highlights

## ðŸ¤ Community

- **Be respectful** and constructive in discussions
- **Help others** learn and contribute
- **Share knowledge** through documentation and examples
- **Celebrate successes** and learn from failures

---

Thank you for contributing to DeepCausalMMM! Together we're building the future of Marketing Mix Modeling ðŸš€
